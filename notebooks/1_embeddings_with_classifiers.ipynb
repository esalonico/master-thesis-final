{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a72f905c",
   "metadata": {},
   "source": [
    "# Embeddings + Classic ML Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd44d317",
   "metadata": {},
   "source": [
    "\n",
    "## 1) Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c311e721",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from classifiers import benchmark_classifiers, make_classifiers\n",
    "from embeddings import EmbeddingCache, OpenAIEmbeddingCache, SPECTER2EmbeddingCache\n",
    "from utils import clean_dataframe, join_title_abstract\n",
    "\n",
    "load_dotenv(\"../.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5bf805",
   "metadata": {},
   "outputs": [],
   "source": [
    "FULL_DATASET_CSV = os.getenv(\"FULL_DATASET_CSV\")\n",
    "LABEL_COL = os.getenv(\"LABEL_COL\")\n",
    "RESULTS_DIR = os.getenv(\"RESULTS_DIR\")\n",
    "RANDOM_STATE = int(os.getenv(\"RANDOM_STATE\"))\n",
    "\n",
    "# embeddings cache directory\n",
    "CACHE_DIR_EMBEDDINGS = \"../.embeddings_cache\"\n",
    "CACHE_DIR_CLASSIFIERS = \"../.classifiers_cache\"\n",
    "\n",
    "# CV\n",
    "N_SPLITS = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7288a5",
   "metadata": {},
   "source": [
    "\n",
    "## 2) Choose embedding models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b0a473",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_MODELS = [\n",
    "    # generalist\n",
    "    \"sentence-transformers/all-MiniLM-L6-v2\",                 # https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2\n",
    "    \"sentence-transformers/all-mpnet-base-v2\",                # https://huggingface.co/sentence-transformers/all-mpnet-base-v2\n",
    "    \"text-embedding-3-small\",                                 # https://platform.openai.com/docs/guides/embeddings\n",
    "    \"text-embedding-3-large\",                                 # https://platform.openai.com/docs/guides/embeddings\n",
    "    \n",
    "    # biomedical/scientifical\n",
    "    \"allenai/specter2_base\",                                  # https://huggingface.co/allenai/specter2_base\n",
    "    \"allenai/biomed_roberta_base\",                            # https://huggingface.co/allenai/biomed_roberta_base\n",
    "    \"pritamdeka/S-PubMedBERT-MS-MARCO\",                       # https://huggingface.co/pritamdeka/S-PubMedBert-MS-MARCO\n",
    "    \"microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract\",   # https://huggingface.co/microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract\n",
    "    \"sentence-transformers/embeddinggemma-300m-medical\"       # https://huggingface.co/sentence-transformers/embeddinggemma-300m-medical\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24f8c89",
   "metadata": {},
   "source": [
    "\n",
    "## 3) Load & clean data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af45431",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = pd.read_csv(FULL_DATASET_CSV, usecols=[\"id\", \"title\", \"abstract\", LABEL_COL])\n",
    "df_raw = df_raw.rename(columns={LABEL_COL: \"label\"})  # rename the label column to \"label\"\n",
    "\n",
    "print(f\"Raw dataset shape: {df_raw.shape}\")\n",
    "df_raw.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818cef49",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = clean_dataframe(df_raw)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16555167",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Rows:\", len(df))\n",
    "\n",
    "# show class imbalance\n",
    "label_names = df[\"label\"].map({True: \"Positive\", False: \"Negative\"})\n",
    "summary = pd.DataFrame({\"count\": label_names.value_counts(), \"percent\": (label_names.value_counts(normalize=True) * 100).round(2)})\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93987836",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = join_title_abstract(df)\n",
    "y = df[\"label\"].values.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847f8dcf",
   "metadata": {},
   "source": [
    "\n",
    "## 4) Compute (and cache) embeddings\n",
    "Embeddings are cached under `CACHE_DIR_EMBEDDINGS` and keyed by model+dataset hash.\n",
    "\n",
    "Store unnormalized embeddings: it's always easy to normalize embeddings later than to \"unnormalize\" already normalized ones.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596ef296",
   "metadata": {},
   "outputs": [],
   "source": [
    "# these settings help with some libraries that use multiple threads by default (specter2)\n",
    "import os\n",
    "\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe5e548",
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_general = EmbeddingCache(cache_dir=CACHE_DIR_EMBEDDINGS)\n",
    "cache_specter = SPECTER2EmbeddingCache(cache_dir=CACHE_DIR_EMBEDDINGS)\n",
    "cache_openai = OpenAIEmbeddingCache(cache_dir=CACHE_DIR_EMBEDDINGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d616d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute embeddings for all models\n",
    "X_by_model = {}\n",
    "for model_name in EMBEDDING_MODELS:\n",
    "    print(f\"Processing model: {model_name}\")\n",
    "\n",
    "    # choose the right cache class\n",
    "    if EmbeddingCache.is_specter2_model(model_name):\n",
    "        cache = cache_specter\n",
    "    elif EmbeddingCache.is_openai_model(model_name):\n",
    "        cache = cache_openai\n",
    "    else:\n",
    "        cache = cache_general\n",
    "\n",
    "    try:\n",
    "        X, meta = cache.compute(\n",
    "            texts,\n",
    "            model_name,\n",
    "            batch_size=128,\n",
    "            normalize_embeddings=False,\n",
    "            device=None,\n",
    "        )\n",
    "        X_by_model[model_name] = X\n",
    "        print(\"Shape:\", X.shape, \"| dim:\", X.shape[1])\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to process {model_name}: {e}\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937974cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize embeddings\n",
    "X_by_model_normalized = {}\n",
    "for model_name, X in X_by_model.items():\n",
    "\n",
    "    # choose the right cache class\n",
    "    if EmbeddingCache.is_specter2_model(model_name):\n",
    "        cache = cache_specter\n",
    "    elif EmbeddingCache.is_openai_model(model_name):\n",
    "        cache = cache_openai\n",
    "    else:\n",
    "        cache = cache_general\n",
    "\n",
    "    X_normalized = cache.normalize(X)\n",
    "    X_by_model_normalized[model_name] = X_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16d37ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding shapes\n",
    "rows = []\n",
    "for model_name, X in X_by_model_normalized.items():\n",
    "    rows.append({\"embedding_model\": model_name, \"shape\": X.shape})\n",
    "\n",
    "shapes_df = pd.DataFrame(rows)\n",
    "shapes_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fef7f90",
   "metadata": {},
   "source": [
    "## 5) Evaluate classifiers (Stratified K-Fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e58fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create classifiers\n",
    "classifiers = make_classifiers(random_state=RANDOM_STATE)\n",
    "classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0107eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\".*pkg_resources is deprecated.*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e775b0",
   "metadata": {},
   "source": [
    "### Normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba33a60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalized embeddings\n",
    "per_fold_df_norm, summary_df_norm = benchmark_classifiers(\n",
    "    X_by_model=X_by_model_normalized,\n",
    "    y=y,\n",
    "    classifiers=classifiers,\n",
    "    n_splits=N_SPLITS,\n",
    "    random_state=RANDOM_STATE,\n",
    "    use_cache=True,\n",
    "    cache_dir=os.path.join(CACHE_DIR_CLASSIFIERS, \"base\"),\n",
    ")\n",
    "summary_df_norm.round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67904ac6",
   "metadata": {},
   "source": [
    "### Unnormalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd65796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unnormalized embeddings\n",
    "per_fold_df_unnorm, summary_df_unnorm = benchmark_classifiers(\n",
    "    X_by_model=X_by_model,\n",
    "    y=y,\n",
    "    classifiers=classifiers,\n",
    "    n_splits=N_SPLITS,\n",
    "    random_state=RANDOM_STATE,\n",
    "    use_cache=True,\n",
    "    cache_dir=os.path.join(CACHE_DIR_CLASSIFIERS, \"unnormalized\"),\n",
    ")\n",
    "summary_df_unnorm.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16169d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export data to generate tables in the paper\n",
    "summary_df_norm.embedding_model = summary_df_norm.embedding_model.str.split(\"/\").str[-1]\n",
    "summary_df_norm = summary_df_norm.round(2)\n",
    "summary_df_norm = summary_df_norm.sort_values(by=\"roc_auc_mean\", ascending=False)[\n",
    "    [\"embedding_model\", \"classifier\", \"roc_auc_mean\", \"recall_mean\", \"precision_mean\", \"specificity_mean\", \"acc_mean\"]\n",
    "]\n",
    "summary_df_norm = summary_df_norm.reset_index(drop=True)\n",
    "\n",
    "summary_df_norm.to_csv(\n",
    "    os.path.join(\n",
    "        RESULTS_DIR,\n",
    "        \"thesis_figures_tables_generation\",\n",
    "        \"1\",\n",
    "        \"summary_normalized_embeddings.csv\",\n",
    "    ),\n",
    "    index=False,\n",
    ")\n",
    "summary_df_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c1476a",
   "metadata": {},
   "source": [
    "\n",
    "## 6) Simple visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c482de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalized embeddingd\n",
    "# plots for PR AUC, Accuracy, Recall, Precision (top 15 combos each)\n",
    "metrics = [\n",
    "    (\"acc_mean\", \"Accuracy\"),\n",
    "    (\"recall_mean\", \"Recall\"),\n",
    "    (\"precision_mean\", \"Precision\"),\n",
    "    (\"specificity_mean\", \"Specificity\"),\n",
    "]\n",
    "\n",
    "summary_df_norm.embedding_model = summary_df_norm.embedding_model.str.split(\"/\").str[-1]\n",
    "\n",
    "\n",
    "# helper to find the column in summary_df_norm (handles names like pr_auc_mean / pr_auc_std etc.)\n",
    "def find_metric_col(df, key):\n",
    "    key = key.lower()\n",
    "    for c in df.columns:\n",
    "        if key in c.lower():\n",
    "            # prefer mean columns if available\n",
    "            if c.lower().endswith(\"_mean\"):\n",
    "                return c\n",
    "            # otherwise just return first match\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "\n",
    "top_k = 8\n",
    "fig, axes = plt.subplots(2, 2, figsize=(18, 10), dpi=150)\n",
    "axes = axes.flatten()\n",
    "\n",
    "bar_height = 0.6  # decrease this value for more spacing (default is ~0.8)\n",
    "\n",
    "for ax, (metric_key, metric_label) in zip(axes, metrics):\n",
    "    col = find_metric_col(summary_df_norm, metric_key)\n",
    "    if col is None:\n",
    "        ax.text(0.5, 0.5, f\"No column found for '{metric_label}'\", ha=\"center\", va=\"center\")\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        continue\n",
    "\n",
    "    summary_sorted = summary_df_norm.sort_values(col, ascending=False).head(top_k)\n",
    "    labels = (summary_sorted[\"embedding_model\"] + \"\\n\" + summary_sorted[\"classifier\"]).tolist()\n",
    "    vals = summary_sorted[col].values\n",
    "\n",
    "    # plot bars with reduced height for more spacing\n",
    "    ax.barh(range(len(vals)), vals, height=bar_height, color=\"C0\")\n",
    "    ax.set_yticks(range(len(vals)))\n",
    "    ax.set_yticklabels(labels, linespacing=1.3)\n",
    "    ax.invert_yaxis()\n",
    "\n",
    "    # force x limits to 0-1\n",
    "    ax.set_xlim(0, 1.0)\n",
    "\n",
    "    # major ticks every 0.1, minor ticks every 0.05\n",
    "    major_xticks = np.arange(0, 1.0001, 0.1)\n",
    "    minor_xticks = np.arange(0, 1.0001, 0.05)\n",
    "    ax.set_xticks(major_xticks)\n",
    "    ax.set_xticks(minor_xticks, minor=True)\n",
    "\n",
    "    # lighter vertical gridlines at 0.05 (minor); keep major ticks without heavy grid\n",
    "    ax.grid(axis=\"x\", which=\"minor\", linestyle=\"--\", color=\"gray\", linewidth=0.5, alpha=0.35)\n",
    "\n",
    "    # data callouts at end of bars\n",
    "    x_offset = (1.0 - 0) * 0.01\n",
    "    for i, v in enumerate(vals):\n",
    "        ax.text(v + x_offset, i, f\"{v:.3f}\", va=\"center\", ha=\"left\", fontsize=9, color=\"black\")\n",
    "\n",
    "    ax.set_xlabel(f\"{metric_label}\" if \"mean\" in col.lower() else metric_label)\n",
    "    # ax.set_title(f\"Top {top_k} combinations by {metric_label.upper()}\")\n",
    "\n",
    "# plt.suptitle(\"NORMALIZED EMBEDDINGS\", fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5cf001f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plots for PR AUC, Accuracy, Recall, Precision (top 15 combos each)\n",
    "metrics = [\n",
    "    # (\"pr_auc\", \"PR AUC\"),\n",
    "    (\"acc_mean\", \"Accuracy\"),\n",
    "    (\"recall_mean\", \"Recall\"),\n",
    "    (\"precision_mean\", \"Precision\"),\n",
    "    (\"specificity_mean\", \"Specificity\"),\n",
    "]\n",
    "\n",
    "summary_df_unnorm.embedding_model = summary_df_unnorm.embedding_model.str.split(\"/\").str[-1]\n",
    "\n",
    "\n",
    "# helper to find the column in summary_df_unnorm (handles names like pr_auc_mean / pr_auc_std etc.)\n",
    "def find_metric_col(df, key):\n",
    "    key = key.lower()\n",
    "    for c in df.columns:\n",
    "        if key in c.lower():\n",
    "            # prefer mean columns if available\n",
    "            if c.lower().endswith(\"_mean\"):\n",
    "                return c\n",
    "            # otherwise just return first match\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "\n",
    "top_k = 10\n",
    "fig, axes = plt.subplots(2, 2, figsize=(18, 10), dpi=150)\n",
    "axes = axes.flatten()\n",
    "\n",
    "for ax, (metric_key, metric_label) in zip(axes, metrics):\n",
    "    col = find_metric_col(summary_df_unnorm, metric_key)\n",
    "    if col is None:\n",
    "        ax.text(0.5, 0.5, f\"No column found for '{metric_label}'\", ha=\"center\", va=\"center\")\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        continue\n",
    "\n",
    "    summary_sorted = summary_df_unnorm.sort_values(col, ascending=False).head(top_k)\n",
    "    labels = (summary_sorted[\"embedding_model\"] + \" | \" + summary_sorted[\"classifier\"]).tolist()\n",
    "    vals = summary_sorted[col].values\n",
    "\n",
    "    # plot bars\n",
    "    ax.barh(range(len(vals)), vals, color=\"C0\")\n",
    "    ax.set_yticks(range(len(vals)))\n",
    "    ax.set_yticklabels(labels)\n",
    "    ax.invert_yaxis()\n",
    "\n",
    "    # force x limits to 0-1\n",
    "    ax.set_xlim(0, 1.0)\n",
    "\n",
    "    # major ticks every 0.1, minor ticks every 0.05\n",
    "    major_xticks = np.arange(0, 1.0001, 0.1)\n",
    "    minor_xticks = np.arange(0, 1.0001, 0.05)\n",
    "    ax.set_xticks(major_xticks)\n",
    "    ax.set_xticks(minor_xticks, minor=True)\n",
    "\n",
    "    # lighter vertical gridlines at 0.05 (minor); keep major ticks without heavy grid\n",
    "    ax.grid(axis=\"x\", which=\"minor\", linestyle=\"--\", color=\"gray\", linewidth=0.5, alpha=0.35)\n",
    "\n",
    "    # data callouts at end of bars\n",
    "    x_offset = (1.0 - 0) * 0.01\n",
    "    for i, v in enumerate(vals):\n",
    "        ax.text(v + x_offset, i, f\"{v:.3f}\", va=\"center\", ha=\"left\", fontsize=9, color=\"black\")\n",
    "\n",
    "    ax.set_xlabel(f\"{metric_label} (mean across folds)\" if \"mean\" in col.lower() else metric_label)\n",
    "    ax.set_title(f\"Top {top_k} combos by {metric_label.upper()}\")\n",
    "\n",
    "plt.suptitle(\"UNNORMALIZED EMBEDDINGS\", fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3302daae",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_df = summary_df_norm.merge(\n",
    "    summary_df_unnorm,\n",
    "    on=[\"embedding_model\", \"classifier\"],\n",
    "    suffixes=(\"_norm\", \"_unnorm\"),\n",
    ").round(3)\n",
    "\n",
    "# compute differences between normalized and unnormalized\n",
    "comparison_df[\"diff_acc\"] = comparison_df[\"acc_mean_norm\"] - comparison_df[\"acc_mean_unnorm\"]\n",
    "comparison_df[\"diff_recall\"] = comparison_df[\"recall_mean_norm\"] - comparison_df[\"recall_mean_unnorm\"]\n",
    "comparison_df[\"diff_precision\"] = comparison_df[\"precision_mean_norm\"] - comparison_df[\"precision_mean_unnorm\"]\n",
    "comparison_df[\"diff_specificity\"] = comparison_df[\"specificity_mean_norm\"] - comparison_df[\"specificity_mean_unnorm\"]\n",
    "\n",
    "comparison_df[[\"diff_acc\", \"diff_recall\", \"diff_precision\", \"diff_specificity\"]].describe().round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb7390d",
   "metadata": {},
   "source": [
    "# 8) Fixed Recall (Sensitivity, TPR) Analysis (95% Recall Target)\n",
    "\n",
    "Now let's implement the threshold tuning approach to fix recall at 95% and see how other metrics perform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f554f661",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.path.join(\"..\", \"src\"))\n",
    "\n",
    "from classifiers.fixed_recall_classifiers import analyze_recall_precision_tradeoff, benchmark_classifiers_fixed_recall, find_recall_threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b352ce",
   "metadata": {},
   "source": [
    "Benchmark all combos with fixed recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06b86ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\".*pkg_resources is *\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "babae6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from classifiers.classifier_cache import ClassifierCache\n",
    "\n",
    "# Create cache for fixed recall classifiers\n",
    "fixed_recall_cache = ClassifierCache(cache_dir=os.path.join(CACHE_DIR_CLASSIFIERS, \"fixed_recall\"))\n",
    "\n",
    "print(\"Running fixed recall evaluation (95% target) for selected models...\")\n",
    "per_fold_df_fixed, summary_df_fixed = benchmark_classifiers_fixed_recall(\n",
    "    X_by_model_normalized,\n",
    "    y,\n",
    "    classifiers,\n",
    "    target_recall=0.95,\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_splits=N_SPLITS,\n",
    "    cache=fixed_recall_cache,\n",
    ")\n",
    "\n",
    "summary_df_fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee96e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export data to generate tables in the paper\n",
    "summary_df_fixed = summary_df_fixed.round(2)\n",
    "summary_df_fixed = summary_df_fixed.sort_values(by=\"specificity_mean\", ascending=False)[\n",
    "    [\"embedding_model\", \"classifier\", \"recall_mean\", \"precision_mean\", \"specificity_mean\", \"acc_mean\", \"threshold_mean\"]\n",
    "]\n",
    "summary_df_fixed = summary_df_fixed.reset_index(drop=True)\n",
    "\n",
    "summary_df_fixed.to_csv(\n",
    "    os.path.join(\n",
    "        RESULTS_DIR,\n",
    "        \"thesis_figures_tables_generation\",\n",
    "        \"1\",\n",
    "        \"summary_fixed_threshold_95.csv\",\n",
    "    ),\n",
    "    index=False,\n",
    ")\n",
    "summary_df_fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace28cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plots for PR AUC, Accuracy, Recall, Precision (top 15 combos each)\n",
    "metrics = [\n",
    "    (\"acc_mean\", \"Accuracy\"),\n",
    "    (\"recall_mean\", \"Recall\"),\n",
    "    (\"precision_mean\", \"Precision\"),\n",
    "    (\"specificity_mean\", \"Specificity\"),\n",
    "]\n",
    "\n",
    "summary_df_fixed[\"embedding_model_short\"] = summary_df_fixed.embedding_model.str.split(\"/\").str[-1]\n",
    "\n",
    "\n",
    "# helper to find the column in summary_df_fixed (handles names like pr_auc_mean / pr_auc_std etc.)\n",
    "def find_metric_col(df, key):\n",
    "    key = key.lower()\n",
    "    for c in df.columns:\n",
    "        if key in c.lower():\n",
    "            # prefer mean columns if available\n",
    "            if c.lower().endswith(\"_mean\"):\n",
    "                return c\n",
    "            # otherwise just return first match\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "\n",
    "top_k = 8\n",
    "fig, axes = plt.subplots(2, 2, figsize=(20, 10), dpi=150)\n",
    "axes = axes.flatten()\n",
    "\n",
    "for ax, (metric_key, metric_label) in zip(axes, metrics):\n",
    "    col = find_metric_col(summary_df_fixed, metric_key)\n",
    "    if col is None:\n",
    "        ax.text(0.5, 0.5, f\"No column found for '{metric_label}'\", ha=\"center\", va=\"center\")\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        continue\n",
    "\n",
    "    summary_sorted = summary_df_fixed.sort_values(col, ascending=False).head(top_k)\n",
    "    labels = (summary_sorted[\"embedding_model_short\"] + \" | \" + summary_sorted[\"classifier\"]).tolist()\n",
    "    vals = summary_sorted[col].values\n",
    "\n",
    "    # plot bars\n",
    "    ax.barh(range(len(vals)), vals, color=\"C0\")\n",
    "    ax.set_yticks(range(len(vals)))\n",
    "    ax.set_yticklabels(labels)\n",
    "    ax.invert_yaxis()\n",
    "\n",
    "    # force x limits to 0-1\n",
    "    ax.set_xlim(0, 1.0)\n",
    "\n",
    "    # major ticks every 0.1, minor ticks every 0.05\n",
    "    major_xticks = np.arange(0, 1.0001, 0.1)\n",
    "    minor_xticks = np.arange(0, 1.0001, 0.05)\n",
    "    ax.set_xticks(major_xticks)\n",
    "    ax.set_xticks(minor_xticks, minor=True)\n",
    "\n",
    "    # lighter vertical gridlines at 0.05 (minor); keep major ticks without heavy grid\n",
    "    ax.grid(axis=\"x\", which=\"minor\", linestyle=\"--\", color=\"gray\", linewidth=0.5, alpha=0.35)\n",
    "\n",
    "    # data callouts at end of bars\n",
    "    x_offset = (1.0 - 0) * 0.01\n",
    "    for i, v in enumerate(vals):\n",
    "        ax.text(v + x_offset, i, f\"{v:.2f}\", va=\"center\", ha=\"left\", fontsize=9, color=\"black\")\n",
    "\n",
    "    ax.set_xlabel(f\"{metric_label}\" if \"mean\" in col.lower() else metric_label)\n",
    "    ax.set_title(f\"Top {top_k} combos by {metric_label.upper()}\")\n",
    "\n",
    "plt.suptitle(\"Fixed Recall Evaluation (Target Recall = 0.95)\", fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53c20cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze recall-precision tradeoff for the best performing model\n",
    "# Use the best model from our fixed recall results (highest precision)\n",
    "best_fixed = summary_df_fixed.iloc[0]  # Best precision in fixed recall results\n",
    "best_embed = best_fixed[\"embedding_model\"]\n",
    "best_clf = best_fixed[\"classifier\"]\n",
    "\n",
    "print(f\"Analyzing recall-precision tradeoff for: {best_embed} + {best_clf}\")\n",
    "\n",
    "# Analyze different recall targets\n",
    "tradeoff_analysis = analyze_recall_precision_tradeoff(\n",
    "    X_by_model_normalized[best_embed],\n",
    "    y,\n",
    "    classifiers[best_clf],\n",
    "    recall_targets=[0.95, 0.96, 0.97, 0.98, 0.99],\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_splits=N_SPLITS,\n",
    ")\n",
    "\n",
    "print(\"\\n=== RECALL-PRECISION TRADEOFF ANALYSIS ===\")\n",
    "tradeoff_analysis.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969b0369",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save tradeoff analysis results\n",
    "tradeoff_analysis_dir = os.path.join(RESULTS_DIR, \"1\", best_embed.replace(\"/\", \"_\"), best_clf)\n",
    "os.makedirs(tradeoff_analysis_dir, exist_ok=True)\n",
    "tradeoff_analysis.to_csv(os.path.join(tradeoff_analysis_dir, \"recall_precision_tradeoff_analysis.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9732e43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export data to generate tables in the paper\n",
    "\n",
    "summary_df_norm = summary_df_norm.round(2)\n",
    "summary_df_norm = summary_df_norm.sort_values(by=\"roc_auc_mean\", ascending=False)[['embedding_model', 'classifier', 'roc_auc_mean', 'recall_mean', \"precision_mean\", \"specificity_mean\", \"acc_mean\"]]\n",
    "summary_df_norm = summary_df_norm.reset_index(drop=True)\n",
    "\n",
    "summary_df_norm.to_csv(\n",
    "    os.path.join(\n",
    "        RESULTS_DIR,\n",
    "        \"thesis_figures_tables_generation\",\n",
    "        \"1\",\n",
    "        \"summary_normalized_embeddings.csv\",\n",
    "    ),\n",
    "    index=False,\n",
    ")\n",
    "summary_df_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872fd65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mticker\n",
    "\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle(f\"Recall-Precision Tradeoff Analysis\\n{best_embed} + {best_clf}\", fontsize=16)\n",
    "\n",
    "# Plot 1: Accuracy vs Target Recall\n",
    "ax1.plot(tradeoff_analysis[\"target_recall\"], tradeoff_analysis[\"accuracy_mean\"], \"bo-\", linewidth=2, markersize=6)\n",
    "ax1.set_xlabel(\"Target Recall\")\n",
    "ax1.set_ylabel(\"Achieved Accuracy\")\n",
    "ax1.set_title(\"Accuracy vs Target Recall\")\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_ylim(0, 1.0)\n",
    "ax1.axvline(x=0.95, color=\"red\", linestyle=\"--\", alpha=0.7, label=\"95% Target\")\n",
    "ax1.legend()\n",
    "\n",
    "# Plot 2: F1 Score vs Target Recall\n",
    "ax2.plot(tradeoff_analysis[\"target_recall\"], tradeoff_analysis[\"f1_mean\"], \"go-\", linewidth=2, markersize=6)\n",
    "ax2.set_xlabel(\"Target Recall\")\n",
    "ax2.set_ylabel(\"F1 Score\")\n",
    "ax2.set_title(\"F1 Score vs Target Recall\")\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_ylim(0, 1.0)\n",
    "ax2.axvline(x=0.95, color=\"red\", linestyle=\"--\", alpha=0.7, label=\"95% Target\")\n",
    "ax2.legend()\n",
    "\n",
    "# Plot 3: Actual vs Target Recall\n",
    "ax3.plot(tradeoff_analysis[\"target_recall\"], tradeoff_analysis[\"actual_recall_mean\"], \"ro-\", linewidth=2, markersize=6)\n",
    "ax3.plot([tradeoff_analysis.target_recall.min(), 0.99], [tradeoff_analysis.target_recall.min(), 0.99], \"k--\", alpha=0.5, label=\"Perfect Match\")\n",
    "ax3.set_xlabel(\"Target Recall\")\n",
    "ax3.set_ylabel(\"Achieved Recall\")\n",
    "ax3.set_title(\"Achieved vs Target Recall\")\n",
    "ax3.grid(True, alpha=0.3)\n",
    "ax3.legend()\n",
    "\n",
    "# Plot 4: Specificity vs Target Recall\n",
    "ax4.plot(tradeoff_analysis[\"target_recall\"], tradeoff_analysis[\"specificity_mean\"], \"mo-\", linewidth=2, markersize=6)\n",
    "ax4.set_xlabel(\"Target Recall\")\n",
    "ax4.set_ylabel(\"Achieved Specificity\")\n",
    "ax4.set_title(\"Specificity vs Target Recall\")\n",
    "ax4.grid(True, alpha=0.3)\n",
    "ax4.axvline(x=0.95, color=\"red\", linestyle=\"--\", alpha=0.7, label=\"95% Target\")\n",
    "ax4.legend()\n",
    "\n",
    "# format x-axis ticks to show two decimals\n",
    "for ax in (ax1, ax2, ax3, ax4):\n",
    "    ax.xaxis.set_major_locator(mticker.MultipleLocator(0.01))\n",
    "    ax.xaxis.set_major_formatter(mticker.FormatStrFormatter(\"%.2f\"))\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (master-thesis)",
   "language": "python",
   "name": "master-thesis-final"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
