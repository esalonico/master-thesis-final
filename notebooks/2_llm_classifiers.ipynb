{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47d29eec",
   "metadata": {},
   "source": [
    "# LLM Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9ee475",
   "metadata": {},
   "source": [
    "## 1) Configuration and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c48430",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "sys.path.append(\"../src\")\n",
    "\n",
    "from llm import GeminiClassifier, MedGemmaClassifier, OpenAIClassifier, evaluate_llm_classifier\n",
    "from utils import clean_dataframe, sample_test_papers\n",
    "\n",
    "load_dotenv(\"../.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eacf5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "FULL_DATASET_CSV = os.getenv(\"FULL_DATASET_CSV\")\n",
    "LABEL_COL = os.getenv(\"LABEL_COL\")\n",
    "RESULTS_DIR = os.getenv(\"RESULTS_DIR\")\n",
    "RANDOM_STATE = int(os.getenv(\"RANDOM_STATE\"))\n",
    "\n",
    "# create results directory if it doesn't exist\n",
    "os.makedirs(f\"{RESULTS_DIR}/2\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cac31ce",
   "metadata": {},
   "source": [
    "## 2) Load and Clean Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796a0e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "df = pd.read_csv(FULL_DATASET_CSV, usecols=[\"id\", \"title\", \"abstract\", LABEL_COL])\n",
    "df = df.rename(columns={LABEL_COL: \"label\"})  # rename the label column to \"label\"\n",
    "\n",
    "# clean datasets\n",
    "df = clean_dataframe(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b905c29b",
   "metadata": {},
   "source": [
    "## 3) Configure LLM Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27685309",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm import criterias\n",
    "\n",
    "# load criteria\n",
    "INCLUSION_CRITERIA = criterias.INCLUSION_CRITERIA\n",
    "EXCLUSION_CRITERIA = criterias.EXCLUSION_CRITERIA\n",
    "\n",
    "# create classifiers\n",
    "classifiers = {\n",
    "    \"medgemma-27b-text-it\": MedGemmaClassifier(),\n",
    "    \"openai_gpt-5-mini\": OpenAIClassifier(model=\"gpt-5-mini\"),\n",
    "    \"openai_gpt-5-nano\": OpenAIClassifier(model=\"gpt-5-nano\"),\n",
    "    \"gemini_gemini-2.5-flash\": GeminiClassifier(model=\"gemini-2.5-flash\"),\n",
    "    \"gemini_gemini-2.5-pro\": GeminiClassifier(model=\"gemini-2.5-pro\"),\n",
    "}\n",
    "\n",
    "# initialize classifiers\n",
    "for classifier in classifiers.values():\n",
    "    classifier.set_criteria(INCLUSION_CRITERIA, EXCLUSION_CRITERIA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458bb1e9",
   "metadata": {},
   "source": [
    "## 4) Evaluation on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d32cbd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split dataframe maintaining same indices as X/y split in other notebooks\n",
    "train_df, test_df = train_test_split(df, test_size=0.20, random_state=RANDOM_STATE, stratify=df[\"label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8265d2f3",
   "metadata": {},
   "source": [
    "1) Compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b52d354",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "\n",
    "# run classification\n",
    "for provider, classifier in classifiers.items():\n",
    "    results_df = classifier.classify_dataframe(test_df, parallel=True, n_workers=15)\n",
    "    results[provider] = results_df\n",
    "    results_df.to_csv(f\"{RESULTS_DIR}/2/{provider}_{len(test_df)}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b36718e",
   "metadata": {},
   "source": [
    "2) Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984f9c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "results[\"openai_gpt-5-mini\"] = pd.read_csv(f\"../results/2/openai_gpt-5-mini_{len(test_df)}.csv\")\n",
    "results[\"openai_gpt-5-nano\"] = pd.read_csv(f\"../results/2/openai_gpt-5-nano_{len(test_df)}.csv\")\n",
    "results[\"gemini_gemini-2.5-flash\"] = pd.read_csv(f\"../results/2/gemini_gemini-2.5-flash_{len(test_df)}.csv\")\n",
    "results[\"gemini_gemini-2.5-pro\"] = pd.read_csv(f\"../results/2/gemini_gemini-2.5-pro_{len(test_df)}.csv\")\n",
    "results[\"medgemma-27b-text-it\"] = pd.read_csv(f\"../results/2/medgemma-27b-text-it_{len(test_df)}.csv\")\n",
    "results.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344da43e",
   "metadata": {},
   "source": [
    "## 6) Evaluate Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a932ee7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# decision distribution and metrics\n",
    "\n",
    "metrics_list = []\n",
    "\n",
    "for name, results_data in results.items():\n",
    "    decision_counts = results_data[\"decision\"].value_counts(normalize=True)\n",
    "\n",
    "    # treat UNCERTAIN as positive (include)\n",
    "    metrics = evaluate_llm_classifier(results_data[\"label\"], results_data[\"decision\"], uncertain_as_positive=True)\n",
    "\n",
    "    row = {\n",
    "        \"recall\": metrics[\"recall\"],\n",
    "        \"specificity\": metrics[\"specificity\"],\n",
    "        \"accuracy\": metrics[\"accuracy\"],\n",
    "        \"precision\": metrics[\"precision\"],\n",
    "        \"no_percentage\": decision_counts.get(\"NO\", 0),\n",
    "        \"yes_percentage\": decision_counts.get(\"YES\", 0),\n",
    "        \"uncertain_percentage\": decision_counts.get(\"UNCERTAIN\", 0),\n",
    "        \"tot_samples\": metrics[\"total_samples\"],\n",
    "    }\n",
    "    metrics_list.append((name, row))\n",
    "\n",
    "metrics_df = pd.DataFrame.from_dict(dict(metrics_list), orient=\"index\").sort_values(by=\"recall\", ascending=False)\n",
    "display(metrics_df.style.format(\"{:.2f}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01085c0",
   "metadata": {},
   "source": [
    "## 7) Compute confusion matrices\n",
    "For openai_gpt-5-mini and medgemma-27b-text-it\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162f669c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export data to generate plots for the thesis\n",
    "import pickle\n",
    "\n",
    "# save results dictionary to pickle file\n",
    "with open(f\"../results/thesis_figures_tables_generation/2/results_summary.pkl\", \"wb\") as f:\n",
    "    pickle.dump(results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67018fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "models_to_plot = [\"openai_gpt-5-mini\", \"medgemma-27b-text-it\"]\n",
    "\n",
    "for ax, model_name in zip(axes, models_to_plot):\n",
    "    results_data = results[model_name]\n",
    "\n",
    "    # Convert decision to binary (treat UNCERTAIN as positive/YES)\n",
    "    y_pred = results_data[\"decision\"].apply(lambda x: 1 if x in [\"YES\", \"UNCERTAIN\"] else 0)\n",
    "    y_true = results_data[\"label\"].astype(int)\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Exclude\", \"Include\"])\n",
    "    disp.plot(ax=ax, cmap=\"Blues\", colorbar=False)\n",
    "    ax.set_title(model_name)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9de4d61",
   "metadata": {},
   "source": [
    "## 8) Compute costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a552b718",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_costs = {}\n",
    "for name, classifier in classifiers.items():\n",
    "    costs = classifier.compute_costs(results[name])\n",
    "    all_costs[classifier.model] = costs\n",
    "\n",
    "all_costs_df = pd.DataFrame(all_costs).T\n",
    "display(all_costs_df.style.format({\"cost_per_paper\": \"$ {:.4f}\", \"cost_per_1k_papers\": \"$ {:.2f}\", \"total_cost\": \"$ {:.3f}\", \"n_papers\": \"{:.0f}\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d17564",
   "metadata": {},
   "source": [
    "## 9) Compute standard deviation\n",
    "To see how \"random\" the LLM responses are (for reproducibility)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5aaf379",
   "metadata": {},
   "source": [
    "1) Compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11713d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_RUNS = 10\n",
    "N_PAPERS_PER_RUN = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68d0ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(f\"{RESULTS_DIR}/2/sd_analysis\", exist_ok=True)\n",
    "\n",
    "for run in tqdm(range(1, N_RUNS + 1), desc=\"Runs\"):\n",
    "    for name, classifier in classifiers.items():\n",
    "        if \"5-mini\" not in name:\n",
    "            continue  # only run for GPT-5-mini to save costs\n",
    "        results_df_run = classifier.classify_dataframe(\n",
    "            df=test_df[0 : min(N_PAPERS_PER_RUN, len(test_df))],\n",
    "            parallel=True,\n",
    "            n_workers=20,\n",
    "        )\n",
    "        results_df_run.to_csv(f\"{RESULTS_DIR}/2/sd_analysis/{classifier.model.split(\"/\")[-1]}_run{run}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12a0c9b",
   "metadata": {},
   "source": [
    "2) Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93cfc9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute metrics means and stddev\n",
    "\n",
    "sd_data = []\n",
    "for file in os.listdir(f\"{RESULTS_DIR}/2/sd_analysis\"):\n",
    "    df = pd.read_csv(os.path.join(f\"{RESULTS_DIR}/2/sd_analysis\", file))\n",
    "    metrics = evaluate_llm_classifier(df[\"label\"], df[\"decision\"], uncertain_as_positive=True)\n",
    "    model_name = file.split(\"_run\")[0]\n",
    "    run_number = int(file.split(\"_run\")[1].split(\".csv\")[0])\n",
    "\n",
    "    data = {\"run\": run_number, \"model\": model_name, **metrics}\n",
    "    sd_data.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aacb8924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyze/display metrics means and stddev\n",
    "\n",
    "sd_df = pd.DataFrame(sd_data)\n",
    "sd_df = sd_df.drop(columns=[\"run\"])\n",
    "\n",
    "means = sd_df.groupby(\"model\").mean()\n",
    "means[\"n_total_runs\"] = N_RUNS\n",
    "\n",
    "stds = sd_df.groupby(\"model\").std()\n",
    "stds.drop(columns=[\"total_samples\", \"total_uncertain\", \"uncertain_rate\"], inplace=True)\n",
    "\n",
    "display(means.style.format(\"{:.2f}\").set_caption(\"Mean\"))\n",
    "display(stds.style.format(\"{:.2f}\").set_caption(\"Standard Deviation\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
